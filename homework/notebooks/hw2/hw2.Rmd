---
title: "R Notebook"
output:
  html_notebook: default
  pdf_document: default
---

[Question 1](#question-1)  
[Question 2](#question-2)  
[Question 3](#question-3)  
[Question 4](#question-4)  
[Question 5](#question-5)  
[Question 6](#question-6)  
[Question 7](#question-7)  


### Question 1

Download the  hw_w2_1.RDat file from canvas and load it into R
```{r}
load(url("https://github.com/jdstokes/PSC204b/raw/master/data/hw_w2_1.rdat"))
head(hw_w2_1)
```
Run a linear regression and check whether there is suppression, and if so, what type?

From the Week 2 lectures slides:  
Suppression variables will increase the predictive validity of an another variable. 

Also, see Cohen and Cohen (1988) pages 84-91

```{r}
y <- scale(hw_w2_1$y, center = TRUE, scale = TRUE)
x1 <- scale(hw_w2_1$x1, center = TRUE, scale = TRUE)
x2 <- scale(hw_w2_1$x2, center = TRUE, scale = TRUE)
```


```{r}
cor(y, x1)
cor(y, x2)
```

Both x1 and x2 are somewhat positively correlated with Y, therefore, we definitely aren't seeing classical suppression.

```{r}
cor(hw_w2_1$y, hw_w2_1$x1)
cor(hw_w2_1$y, hw_w2_1$x2)
```
Neither beta coefficients show opposite signs to the respecific correlation coefficients, so we aren't seeing net suppression. But with both coefficients exceeding the correlation scores above, we this is cooperative suppression.

```{r}
coef(lm(y ~ x1+ x2))
```
As, we can see both x1 and x2 are negatively correlated. 
```{r}
cor(x1, x2)
```
Hmm

```{r}
GetSuppression <- function(y,x1,x2) {
R2.x1 <- summary(lm(y ~ x1))$r.squared
R2.x2 <- summary(lm(y ~ x2))$r.squared
R2.full <- summary(lm(y ~ x1 + x2))$r.squared

if(R2.full > (R2.x1 + R2.x2)){
  print("Yep, there is suppression")
}else {
  print("No suppression")
}
}

GetSuppression.type <-function(y,x1,x2){
r.yx2 <- cor(y,x2)
r.yx1 <- cor(y,x1)
r.x1x2 <-cor(x1,x2)

}

```

```{r}
GetSuppression(hw_w2_1$y,hw_w2_1$x1,hw_w2_1$x2)
GetSuppression.type(hw_w2_1$y,hw_w2_1$x1,hw_w2_1$x2)
```
### Question 2

Download the  hw_w2_2.RDat file from canvas and load it into R  
```{r}
load(url("https://github.com/jdstokes/PSC204b/raw/master/data/hw_w2_2.rdat"))
head(hw_w2_2)
```
Fit a quadratic model and center at the beginning of the study, at the grand-mean and at the end. 
```{r}
# To fit a quadratic model we can just use the lm() function again and add an aditional quadratic (or squared) term.
# Two ways to go about this:
# 1)You could create the squared variable first
hw_w2_2$x.2 <- hw_w2_2$x^2
model.2 <- lm(formula =y ~ x + x.2,data=hw_w2_2)
summary(model.2)

# 2)Alternatively you could incorportate it directly into lm() formula argument using the I() function
model.2 <- lm(formula =y ~ x + I(x^2),data=hw_w2_2)
summary(model.2)
```


```{r}
# Let's go ahead and re center our data around the beginning, mean and end.
hw_w2_2$x.c_gm <- hw_w2_2$x- mean(hw_w2_2$x)
hw_w2_2$x.c_b  <- hw_w2_2$x- min(hw_w2_2$x)
hw_w2_2$x.c_e  <- hw_w2_2$x- max(hw_w2_2$x)

model.2.c_gm <- lm(formula =y ~ x.c_gm + I(x.c_gm^2),data=hw_w2_2)
model.2.c_b <- lm(formula =y ~ x.c_b + I(x.c_b^2),data=hw_w2_2)
model.2.c_e <- lm(formula =y ~ x.c_e + I(x.c_e^2),data=hw_w2_2)

```

Let's compare the centered x value model fits to the original.
```{r}
summary(model.2)$r.squared
summary(model.2.c_gm)$r.squared
summary(model.2.c_b)$r.squared
summary(model.2.c_e)$r.squared
```
As you can see, we find the same R-squared value for all four models. However, one thing thing that has changed are are the estimated intercept and first order paramters.

```{r}
summary(model.2)$coefficients
summary(model.2.c_gm)$coefficients
summary(model.2.c_b)$coefficients
summary(model.2.c_e)$coefficients
```

Centering around the mean or moving the x values closer to zero in polynomial regression may serve to reduce computational inaccuracies caused by multicollinearity among the first and second order terms. You can see that the predicted X estimate has changed and the standard error of the estimate was reduced in the mean centered example.


Next we were asked to plot the figure with the fitted line and the simple slope (or instantaneous rate of change) at the three centering points. So, calculating the instantaneous rate of change at x=0 is super easy because the first derivative in this case is the first order parameter estimate. 
```{r}
# Simple slope at x = grand mean? Well in our grand mean centered model, x=0 is the grand mean.
slope.gm <- coef(model.2.c_gm)[2]

# Simple slope at x = beginning? Well in our grand mean centered model, x=0 is the beginning.
slope.b  <- coef(model.2.c_b)[2]

# And so on for the end point
slope.e  <- coef(model.2.c_e)[2]
```


Now, we can plot these lines over our raw dataset
```{r}
x <- hw_w2_2$x
y <- hw_w2_2$y

x2 <- x^2
model.2b <- lm(formula =y ~ x + x2)
beta <- coef(model.2b)

plot(x,y,
     xlab = "X",
     ylab = "Y",
     pch  = 20,
     cex  = 2,
     col  = "grey")

# Mean point, simple slope:
x.instant <- mean(x)
simpleSlope <- slope.gm # simpleSlope <- beta[2]+2*beta[3]*x.instant
y.instant <- beta[1] + beta[2]*x.instant + beta[3]*x.instant^2
intercept <-  y.instant -simpleSlope*x.instant
abline(intercept,simpleSlope)
x.pred <- seq(min(x), max(x))
y.pred <- predict(model.2,list(x=x.pred, x2=x.pred^2))
lines(x.pred , y.pred, lwd = 3)
points(x.instant,y.instant)


# Beg point, simple slope:
x.instant <- min(x)
simpleSlope <- slope.b # simpleSlope <- beta[2]+2*beta[3]*x.instant
y.instant <- beta[1] + beta[2]*x.instant + beta[3]*x.instant^2
intercept <-  y.instant -simpleSlope*x.instant
abline(intercept,simpleSlope)
x.pred <- seq(min(x), max(x))
y.pred <- predict(model.2,list(x=x.pred, x2=x.pred^2))
lines(x.pred , y.pred, lwd = 3)
points(x.instant,y.instant)


# End point, simple slope:
x.instant <- max(x)
simpleSlope <- slope.e # simpleSlope <- beta[2]+2*beta[3]*x.instant
y.instant <- beta[1] + beta[2]*x.instant + beta[3]*x.instant^2
intercept <-  y.instant -simpleSlope*x.instant
abline(intercept,simpleSlope)
x.pred <- seq(min(x), max(x))
y.pred <- predict(model.2,list(x=x.pred, x2=x.pred^2))
lines(x.pred , y.pred, lwd = 3)
points(x.instant,y.instant)

```

### Question 3  
In this exercise you will simulate two variables that are statistically independent of each other to see what happens when we run a regression of one on the other.  
a) First generate 1000 data points from a normal distribution with mean 0 and standard deviation 1 by typing var1 <- rnorm(1000,0,1) in R. Generate another variable in the same way (call it var2). Run a regression of one variable on the other. Is the slope coefficient statistically significant?  
b) Now run a simulation repeating this process 100 times. This can be done using a loop. From each simulation, save the z-score (the estimated coefficient of var1 divided by its standard error). If the absolute value of the z-score exceeds 2, the estimate is statistically significant. Here is code to perform the simulation:  

```{r}
library(arm)
z.scores <- rep (NA, 100)
for (k in 1:100) {
  var1 <- rnorm (1000,0,1)
  var2 <- rnorm (1000,0,1)
  fit <- lm (var2 ~ var1)
  z.scores[k] <- coef(fit)[2]/se.coef(fit)[2]
}

hist(z.scores)
abline(v=2,col=4,lty=2)
```


How many of these 100 z-scores are statistically significant?  
```{r}
sum(z.scores > 2)

```

### Question 4  
Download the child.iq.dta dataset. You have access to children’s test scores at age 3, mother’seducation, and the mother’s age at the time she gave birth for a sample of 400
children. The data are a Stata file which you can read into R by saving in your working directory and then typing the following:

```{r}
library ("foreign")
data.child <- read.dta("https://github.com/jdstokes/PSC204b/raw/master/data/child.iq.dta")
head(data.child)
```


a)	Fit a regression of child test scores on mother’s age, display the data and fitted model, check assumptions, and interpret the slope coefficient. When do you recommend mothers should give birth? What are you assuming in making these recommendations?  


```{r}
model<-lm(data=data.child,formula=ppvt~momage)
summary(model)
# PlotSimple(y=data.child$ppvt,x=data.child$momage)

```

Check if the mean of the residuals is zero
```{r}
mean(model$residuals)
```
```{r}
par(mfrow = c(2, 2))
plot(model)
```



b)	Repeat this for a regression that further includes mother’s education, interpreting both slope coefficients in this model. Have your conclusions about the timing of birth changed?  


```{r}
model<-lm(data=data.child,formula=ppvt~momage+educ_cat)
summary(model)
```


### Question 5  
5. Download  data from Hamermesh and Parker (2005) on student
evaluations of instructors’ beauty and teaching quality for several courses at the
University of Texas. The teaching evaluations were conducted at the end of the
semester, and the beauty judgments were made later, by six students who had
not attended the classes and were not aware of the course evaluations.

```{r}
# load(url("https://github.com/jdstokes/PSC204b/raw/master/data/TeachingRatings.rda"))

load("/Users/jdstokes/repos/204b/data/TeachingRatings.rda")
head(TeachingRatings)
```

```{r}
colnames(TeachingRatings)
```


a)	Run a regression using beauty to predict course evaluations (eval), controlling for various other inputs. Display the fitted model graphically, and explaining the meaning of each of the coefficients. Plot the residuals versus fitted values.


```{r}
model <- lm(eval ~ minority + age + gender + students,data = TeachingRatings)
summary(model)
```


b)	Fit some other models, including beauty and also other input variables. Consider at least one model with interactions. For each model, state what the predictors are, and what the inputs are (see Section 2.1), and explain the meaning of each of its coefficients.

### Question 6  
6 The data wfw90.dat is from the Work, Family, and Well-Being Survey (Ross, 1990). Pull out the data on earnings, sex, height, and weight.  

```{r}
data = read.table("https://raw.githubusercontent.com/jdstokes/PSC204b/master/data/wfw90.dat", header = T)
head(data)
```

a)	Fit a linear regression model predicting earnings from height. What transformation should you perform in order to interpret the intercept from this model as average earnings for people with average height?  
```{r}
model <- lm(earnings ~ height, data = data)
summary(model)
```

b)	Fit some regression models with the goal of predicting earnings from some combination of sex, height, and weight. Be sure to try various transformations and interactions that might make sense. Choose your preferred model and justify.  
c)	Interpret all model coefficients.  

### Question 7  
An economist runs a regression examining the relations between the average price
of cigarettes, P , and the quantity purchased, Q, across a large sample of counties
in the United States, assuming the following functional form, log Q = α+β log P .
Suppose the estimate for β is 0.3. Interpret this coefficient.  





