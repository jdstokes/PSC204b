---
title: "cross validation"
output: html_notebook
---

Overfitting: When the model is trained too well. So the model is super accurate when predicting the training data but less so with new data.

Underfitting: Model does not fit the training data. The model might be too simple or misspecified due to the omission of important variables.


Simple cross validation example

```{r}
library(mlbench)
library(dplyr)
# Pima Indians Diabetes Database
data(PimaIndiansDiabetes)
dat <- PimaIndiansDiabetes
dim(dat)
levels(dat$diabetes)
head(dat)
```
We can add a column that will let us easily split our data. We'll also go ahead convert our predictor to numeric
```{r}
k <- 4
n <- dim(dat)[1]
dat$foldNum <- sample(rep(1:4,length.out = n))
dat$d2[dat$diabetes == "pos"] = 1
dat$d2[dat$diabetes == "neg"] = 0
```

This section loops through the data k number of times, splitting data into Train and Test batches, modeling the Train data, predicting for both Train and Test data separately, and then calculating a simple accuracy measure.
```{r}
predResults <- data.frame(foldNum=1:k,accTrain=rep(0, k),accTest=rep(0, k))

for(i in 1:k){
testNum <- i

dataTrain <- dat %>%
  filter(foldNum != testNum)

dataTest <- dat %>%
  filter(foldNum == testNum)

fit <- glm(d2 ~ glucose + pedigree + age,family="binomial"(link="logit"), data = dataTrain)

predTrain <- predict(fit, newdata=dataTrain,type = 'response')
predTest <- predict(fit, newdata=dataTest,type = 'response')

#Let's just make our cutoff at .5 for this example
predTrain <- ifelse(predTrain > .5,1,0)
predTest <- ifelse(predTest > .5,1,0)

confusionMatrixTrain<-table(dataTrain$d2,predTrain)
predResults$accTrain[i] <- sum(diag(confusionMatrixTrain))/sum(confusionMatrixTrain)

confusionMatrixTest<-table(dataTest$d2,predTest)
predResults$accTest[i] <- sum(diag(confusionMatrixTest))/sum(confusionMatrixTest)
}
```

Now, we can just calculate the mean accuracies
```{r}
meanAccTrain <-mean(predResults$accTrain)
meanAccTest <-mean(predResults$accTest)

meanAccTrain
meanAccTest
```

